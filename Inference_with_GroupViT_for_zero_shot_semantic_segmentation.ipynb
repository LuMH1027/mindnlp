{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GroupViT/Inference_with_GroupViT_for_zero_shot_semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFir-bFDdQ6-"
   },
   "source": [
    "# Inference with GroupViT: zero-shot semantic segmentation\n",
    "\n",
    "You probably know [CLIP](https://huggingface.co/docs/transformers/model_doc/clip), the famous work by OpenAI that was trained on 400 million (image, text) pairs in a contrastive way to match image with text. That way, the model is (among other things) capable of **zero-shot image classification** (which means, classifying images without requiring any label), by simply casting image classification as the task of pairing an image with the appropriate text.\n",
    "\n",
    "Let me introduce you to a new model, [GroupViT](https://twitter.com/xiaolonw/status/1501239921503129600), which does something very simillar to CLIP, but for the case of **zero-shot semantic segmentation**. Semantic segmentation is the task of labeling every pixel of an image with a certain class (like \"sky\", \"person\", etc.). Typically, one trains a model on a labeled dataset (classical supervised learning). But creating labels for semantic segmentation is a very time-consuming task, as humans need to annotate each pixel of an image with a certain class.\n",
    "\n",
    "Enter GroupViT: the model is trained, similar to CLIP, on a lot of (image, text) pairs, and after training, the model is capable of recognizing groups of certain semantic categories in an image, just by linking the name of the semantic category to the image.\n",
    "\n",
    "Paper: https://arxiv.org/abs/2202.11094\n",
    "\n",
    "ðŸ¤— Docs: https://huggingface.co/docs/transformers/main/en/model_doc/groupvit\n",
    "\n",
    "Let's illustrate this with an example!\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "First, let's install ðŸ¤— HuggingFace Transformers and HuggingFace Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNKeEHdGdSnw"
   },
   "source": [
    "## Load model and processor\n",
    "\n",
    "Here we load the model as well as the processor from the [HuggingFace Hub](https://huggingface.co/). Note that NVIDIA has released several checkpoints, all of which can be found [here](https://huggingface.co/models?other=groupvit). Be sure to play around with them!\n",
    "\n",
    "We'll also load the processor, which can be used to prepare images + texts for the model. \n",
    "\n",
    "We move the model to GPU if it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T15:43:05.709251Z",
     "start_time": "2024-09-27T15:42:57.612367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d1b92d33353f456099ba56b030af09ef",
      "3a4d4dab12c74262b8cac2da05000e58",
      "14969d3557544f799e32afe4678173f2",
      "3d82c236f2f94dcba84e69758573a487",
      "10a57f47db4f46a2a6aa486ed6898b62",
      "949f5a1292cd47d4b68ce46851ac2a2b",
      "db57a2329a7a4fdba7ebf25889e3c001",
      "1a66f6f17c8942ef8a519931b8bbe734",
      "3734ad1c59fa4fadbd2d5c3a42fa5751",
      "e835074ad04c4874a51a98b93ad323de",
      "31dce17150b44303a01e4bdfba2dd711"
     ]
    },
    "id": "eNDHIf4DXzDI",
    "outputId": "c305164d-2cb0-479e-f455-b5a5093c0d2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.645 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoProcessor, GroupViTModel\n",
    "\n",
    "# load model + processor\n",
    "model_name = \"nvidia/groupvit-gcc-yfcc\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T15:44:57.881034Z",
     "start_time": "2024-09-27T15:44:57.245115Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mindnlp.transformers.models.chinese_clip'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m processor \u001B[38;5;241m=\u001B[39m \u001B[43mAutoProcessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mindnlp/mindnlp/transformers/models/auto/processing_auto.py:329\u001B[0m, in \u001B[0;36mAutoProcessor.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m     processor_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessor_class\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m processor_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 329\u001B[0m     processor_class \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor_class_from_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessor_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m processor_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m processor_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    333\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m    334\u001B[0m         trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    336\u001B[0m     )\n",
      "File \u001B[0;32m~/mindnlp/mindnlp/transformers/models/auto/processing_auto.py:126\u001B[0m, in \u001B[0;36mprocessor_class_from_name\u001B[0;34m(class_name)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_name \u001B[38;5;129;01min\u001B[39;00m processors:\n\u001B[1;32m    124\u001B[0m     module_name \u001B[38;5;241m=\u001B[39m model_type_to_module_name(module_name)\n\u001B[0;32m--> 126\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodule_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmindnlp.transformers.models\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, class_name)\n",
      "File \u001B[0;32m~/anaconda3/envs/ms/lib/python3.9/importlib/__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:984\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mindnlp.transformers.models.chinese_clip'"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T15:52:09.732999Z",
     "start_time": "2024-09-27T15:52:03.872644Z"
    }
   },
   "source": [
    "model = GroupViTModel.from_pretrained(model_name)\n",
    "\n",
    "# move to GPU\n",
    "\n",
    "model"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GroupViTModel:\n\tsize mismatch for logit_scale: copying a param with shape () from checkpoint, the shape in current model is (1,).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mGroupViTModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# move to GPU\u001B[39;00m\n\u001B[1;32m      5\u001B[0m model\n",
      "File \u001B[0;32m~/mindnlp/mindnlp/transformers/modeling_utils.py:3000\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2990\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2991\u001B[0m         set_default_dtype(dtype_orig)\n\u001B[1;32m   2993\u001B[0m     (\n\u001B[1;32m   2994\u001B[0m         model,\n\u001B[1;32m   2995\u001B[0m         missing_keys,\n\u001B[1;32m   2996\u001B[0m         unexpected_keys,\n\u001B[1;32m   2997\u001B[0m         mismatched_keys,\n\u001B[1;32m   2998\u001B[0m         offload_index,\n\u001B[1;32m   2999\u001B[0m         error_msgs,\n\u001B[0;32m-> 3000\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3001\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3002\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3003\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3004\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3005\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3006\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3007\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3008\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3009\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3011\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3012\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3013\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mms_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3014\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3015\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3017\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   3018\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/mindnlp/mindnlp/transformers/modeling_utils.py:3389\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001B[0m\n\u001B[1;32m   3385\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize mismatch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m error_msg:\n\u001B[1;32m   3386\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   3387\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3388\u001B[0m         )\n\u001B[0;32m-> 3389\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merror_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unexpected_keys) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   3392\u001B[0m     archs \u001B[38;5;241m=\u001B[39m [] \u001B[38;5;28;01mif\u001B[39;00m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39marchitectures \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39marchitectures\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for GroupViTModel:\n\tsize mismatch for logit_scale: copying a param with shape () from checkpoint, the shape in current model is (1,).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCch0sWndUH0"
   },
   "source": [
    "## Prepare image\n",
    "\n",
    "Here, we load an image from the ADE20k dataset (actually called the [MIT Scene Parsing Benchmark](http://sceneparsing.csail.mit.edu/)), an important benchmark for semantic segmentation. We also load its corresponding ground truth segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "S5vS_DW3ZBZC",
    "outputId": "5cc38587-c15a-4767-96ae-1d2103d2854e"
   },
   "outputs": [],
   "source": [
    "from mindnlp.dataset import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# load image + ground truth map\n",
    "ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n",
    "image = Image.open(ds[0][\"file\"])\n",
    "display(image)\n",
    "segmentation_map = Image.open(ds[1][\"file\"])\n",
    "image = image.resize((224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am_Q9uKcK6U_"
   },
   "source": [
    "GroupViT learns to match semantic categories to pixels.\n",
    "\n",
    "First, we'll collect the 150 ADE20k classes for which the model can find a match with the pixels of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljf3OOzqolZm"
   },
   "outputs": [],
   "source": [
    "def ade_class():\n",
    "    \"\"\"ADE20K classes names.\"\"\"\n",
    "    return [\n",
    "         'wall', 'building', 'sky', 'floor', 'tree', 'ceiling', 'road', 'bed ',\n",
    "        'windowpane', 'grass', 'cabinet', 'sidewalk', 'person', 'earth',\n",
    "        'door', 'table', 'mountain', 'plant', 'curtain', 'chair', 'car',\n",
    "        'water', 'painting', 'sofa', 'shelf', 'house', 'sea', 'mirror', 'rug',\n",
    "        'field', 'armchair', 'seat', 'fence', 'desk', 'rock', 'wardrobe',\n",
    "        'lamp', 'bathtub', 'railing', 'cushion', 'base', 'box', 'column',\n",
    "        'signboard', 'chest of drawers', 'counter', 'sand', 'sink',\n",
    "        'skyscraper', 'fireplace', 'refrigerator', 'grandstand', 'path',\n",
    "        'stairs', 'runway', 'case', 'pool table', 'pillow', 'screen door',\n",
    "        'stairway', 'river', 'bridge', 'bookcase', 'blind', 'coffee table',\n",
    "        'toilet', 'flower', 'book', 'hill', 'bench', 'countertop', 'stove',\n",
    "        'palm', 'kitchen island', 'computer', 'swivel chair', 'boat', 'bar',\n",
    "        'arcade machine', 'hovel', 'bus', 'towel', 'light', 'truck', 'tower',\n",
    "        'chandelier', 'awning', 'streetlight', 'booth', 'television receiver',\n",
    "        'airplane', 'dirt track', 'apparel', 'pole', 'land', 'bannister',\n",
    "        'escalator', 'ottoman', 'bottle', 'buffet', 'poster', 'stage', 'van',\n",
    "        'ship', 'fountain', 'conveyer belt', 'canopy', 'washer', 'plaything',\n",
    "        'swimming pool', 'stool', 'barrel', 'basket', 'waterfall', 'tent',\n",
    "        'bag', 'minibike', 'cradle', 'oven', 'ball', 'food', 'step', 'tank',\n",
    "        'trade name', 'microwave', 'pot', 'animal', 'bicycle', 'lake',\n",
    "        'dishwasher', 'screen', 'blanket', 'sculpture', 'hood', 'sconce',\n",
    "        'vase', 'traffic light', 'tray', 'ashcan', 'fan', 'pier', 'crt screen',\n",
    "        'plate', 'monitor', 'bulletin board', 'shower', 'radiator', 'glass',\n",
    "        'clock', 'flag'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CyMoXSFsaZN"
   },
   "source": [
    "Next, we prepare the image + texts for the model. We basically create 150 image-text pairs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq6PgfBUdqex"
   },
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[f\"a photo of a {word}\" for word in ade_class()],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mUX1rLoscz5",
    "outputId": "3b10e5e1-434f-4aac-f4b9-a242dcfe8935"
   },
   "outputs": [],
   "source": [
    "for k,v in inputs.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLY4VxCsdVL6"
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "Next, let's forward the `pixel values` and the `input_ids` through the model in order to obtain the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7aSF2ynZNgl"
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs, output_segmentation=True)\n",
    "logits = outputs.segmentation_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OI7144HdXNj"
   },
   "source": [
    "## Visualize\n",
    "\n",
    "Finally, let's visualize the segmentation map as predicted by the model. Semantic segmentation datasets typically define what is called a \"palette\", that defines the RGB values to use for each of the classes. \n",
    "\n",
    "The ADE20k dataset consists of 150 classes, so the list below has 150 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXvJ_UlmisIQ"
   },
   "outputs": [],
   "source": [
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
    "            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n",
    "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
    "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n",
    "            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n",
    "            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n",
    "            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n",
    "            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n",
    "            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n",
    "            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n",
    "            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n",
    "            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n",
    "            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n",
    "            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n",
    "            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n",
    "            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n",
    "            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n",
    "            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n",
    "            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n",
    "            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n",
    "            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n",
    "            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n",
    "            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n",
    "            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n",
    "            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n",
    "            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n",
    "            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n",
    "            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n",
    "            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n",
    "            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n",
    "            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n",
    "            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n",
    "            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n",
    "            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n",
    "            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n",
    "            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n",
    "            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n",
    "            [102, 255, 0], [92, 0, 255]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiocVqCPLPxo"
   },
   "source": [
    "The model outputs logits of shape `(batch_size, num_labels, height/4, width/4)`.  We first rescale the logits to match the original size of the image using bilinear interpolation. Next, we perform an argmax on the class dimension, and we create a color map which we draw over the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "CXfnL9Vsanm-",
    "outputId": "abca0561-de60-4e66-cb83-23d050a75d99"
   },
   "outputs": [],
   "source": [
    "from mindnlp.core import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, rescale logits to original image size\n",
    "logits = nn.functional.interpolate(logits.detach().cpu(),\n",
    "                size=image.size[::-1], # (height, width)\n",
    "                mode='bilinear',\n",
    "                align_corners=False)\n",
    "\n",
    "# Second, apply argmax on the class dimension\n",
    "seg = logits.argmax(dim=1)[0]\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18NJ_uAWszVJ"
   },
   "source": [
    "The model is capable of doing this without ever being trained on human annotated semantic datasets.\n",
    "\n",
    "Amazing, isn't it? It shows the potential of what language can do to improve computer vision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-iP9zoxpKkN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Inference with GroupViT for zero shot semantic segmentation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "14aaaada99069094a62f30ac4451511342c5df1a2b82c64600cab3912ac55da9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10a57f47db4f46a2a6aa486ed6898b62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14969d3557544f799e32afe4678173f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a66f6f17c8942ef8a519931b8bbe734",
      "max": 4642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3734ad1c59fa4fadbd2d5c3a42fa5751",
      "value": 4642
     }
    },
    "1a66f6f17c8942ef8a519931b8bbe734": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31dce17150b44303a01e4bdfba2dd711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3734ad1c59fa4fadbd2d5c3a42fa5751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a4d4dab12c74262b8cac2da05000e58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_949f5a1292cd47d4b68ce46851ac2a2b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db57a2329a7a4fdba7ebf25889e3c001",
      "value": "Downloading: 100%"
     }
    },
    "3d82c236f2f94dcba84e69758573a487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e835074ad04c4874a51a98b93ad323de",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_31dce17150b44303a01e4bdfba2dd711",
      "value": " 4.53k/4.53k [00:00&lt;00:00, 87.3kB/s]"
     }
    },
    "949f5a1292cd47d4b68ce46851ac2a2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1b92d33353f456099ba56b030af09ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a4d4dab12c74262b8cac2da05000e58",
       "IPY_MODEL_14969d3557544f799e32afe4678173f2",
       "IPY_MODEL_3d82c236f2f94dcba84e69758573a487"
      ],
      "layout": "IPY_MODEL_10a57f47db4f46a2a6aa486ed6898b62"
     }
    },
    "db57a2329a7a4fdba7ebf25889e3c001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e835074ad04c4874a51a98b93ad323de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
